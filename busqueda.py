# -*- coding: utf-8 -*-
"""Tipo de Lugar Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oaiJg8d8WrezW4t9h0fhtksRbOWEDfiq

# 0. Importando Librerias
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import spacy
from wordcloud import WordCloud
# Bibliotecas para manipulación y visualización de datos
import seaborn as sns
import time
from sklearn.metrics import f1_score

# Modelos y herramientas de Sklearn
from sklearn.svm import SVR
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score
from sklearn.pipeline import Pipeline

nltk.download('stopwords')

"""# 1. Importando el dataset"""

url = "https://raw.githubusercontent.com/gmauricio-toledo/NLP-LCC/main/Rest-Mex/Rest-Mex_2025_train.csv"
df = pd.read_csv(url)
df.head()

df2 = df[['Title','Review','Region','Type']]
df2

"""# 2. Explorando el dataset

## 2.0 Generalizaciones
"""

df2.info()

"""Son 200,000 datos todos del tipo objeto

## 2.1 Valores nulos y duplicados
"""

print("Valores nulos\n", df2.isna().sum())

print("Valores repetidos\n", df2.duplicated(keep=False).sum())

print("Entradas repetidas de las reviews\n",
      df2['Review'].duplicated(keep=False).sum())

print("Entradas repetidas de todo el dataset\n",
      df2.duplicated(keep=False).sum())

print("Entradas repetidas de las reviews que no estan en todo el dataset:",
      df2['Review'].duplicated(
          keep=False).sum() - df2.duplicated(keep=False).sum())

"""Nota: Aquí se puede ver que es diferente el número de repetidos de todo el df y de la columna de Review"""

df_dup = df2[df2.duplicated(keep=False)]

df_review_dup = df2[df2['Review'].duplicated(keep=False)]

indexes = df_review_dup.index

outliers_dup_ind = indexes.difference(df_dup.index)

df2[df2.index.isin(outliers_dup_ind)].sort_values('Review').head(9)

"""Por lo que es importante que eliminemos duplicados en base a la Review

## 2.2 Distribución de los datos
"""

df2['Type'].unique()

sns.countplot(x=df2['Type'], stat='probability')
plt.title('Distribuciones')
plt.xlabel('Tipos de lugar')
plt.ylabel('Total')
plt.show()

sns.countplot(x=df2['Region'].apply(lambda x: x[:2].upper()), stat='probability')
plt.title('Distribuciones')
plt.xlabel('Tipos de lugar')
plt.ylabel('Total')
plt.show()

review_length = df2['Review'].apply(lambda x: len(x.split()))

title_length = df2['Title'].astype(str).apply(lambda x: len(x.split()))

sns.histplot(review_length, bins=50, kde=True)
plt.title('Distribución de la Longitud de Reseñas')
plt.xlabel('Palabras en la reseña')
plt.ylabel('Frecuencia')
plt.show()

""">**TODO**: Hacer algo con el hecho de que hay reseñas muy pequeñas"""

sns.histplot(title_length, bins=30, kde=True)
plt.title('Distribución de la Longitud de Titulos')
plt.xlabel('Palabras en la reseña')
plt.ylabel('Frecuencia')

"""# 3. Preprocesar"""

features = df2.drop('Type', axis=1)
target = df2['Type']

X_train, X_test, y_train, y_test= train_test_split(features, target, test_size=0.2, stratify=target, random_state=64)

"""## 3.0 Convertir a string"""

X_train = X_train.map(str)

"""## 3.1 Tratar los valores nulos y duplicados"""

X_train.loc[:, 'Title'] = X_train['Title'].fillna(' ')

X_train.loc[:, 'Review'] = X_train['Review'].drop_duplicates(keep='last')

X_train = X_train.dropna()
y_train = y_train[X_train.index]

"""## 3.2 Tratar el titulo y las reviews como una columna"""

X_train.insert(loc=2, column='Text', value=X_train['Title'] + ' ' + X_train['Review'])

str(X_train[['Review']].iloc[18])

str(X_train[['Review']].iloc[18]).encode('latin-1').decode()

"""## 3.3 Tratar valores mal codificados"""

def codificar_decodificar_latin(string):
    global posibles_errores
    try:
        new_string = string.encode('latin-1').decode('utf-8')
        posibles_errores +=1
        before_after.append((string, new_string))
        return new_string
    except UnicodeDecodeError as e: # Si no se pudo decodificar del latin, es decir que no esta mal formateado para este encoding
        return string

before_after = []
posibles_errores = 0

X_train['Text'] = X_train['Text'].apply(codificar_decodificar_latin)

posibles_errores

before_after[:2]

"""> En estos dos ejemplos hay uno que si ayudó, y en el otro no. Esto es porque el segundo texto no tiene "caracteres especiales" como acentos, signos, etc..., por lo que se pudo decodificar del latín sin problemas

## 3.4 Convertir a minusculas
"""

X_train['Text'] = X_train['Text'].apply(lambda entry: entry.lower())

"""## 3.5 Tokenizar"""

nlp = spacy.blank(name='es')

tokenized_X_train = [[t.text for t in tok_doc if
          not t.is_punct and \
          not t.is_space and \
          t.is_alpha] for tok_doc in nlp.pipe(X_train['Text']) ]

print(len(tokenized_X_train))
print(y_train.shape)

stopwords = nltk.corpus.stopwords.words('spanish')
stopwords.extend(['\n', '\b', 'si'])

tokenized_X_train = [[w for w in doc if w not in stopwords] for doc in tokenized_X_train]

docs_train = [" ".join(doc) for doc in tokenized_X_train]

wc = WordCloud(background_color="white", max_words=200, contour_width=3, contour_color='steelblue')
wordcloud = wc.generate(' '.join(docs_train[:100000]))

print(wordcloud.words_.keys())

"""# 4. Modelos"""

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.linear_model import Lasso

rfc = RandomForestClassifier(random_state=9)
logreg = LogisticRegression(random_state=9)
gbc = GradientBoostingClassifier(random_state=9)
svc = SVC(probability=True, random_state=9)
lasso_clf = LogisticRegression(penalty='l1', solver='saga', random_state=9)

"""## 4.1 Busqueda Inicial del mejor modelo"""

# Probar en ocotillo
models = {
    "Logistic Regression": logreg,
    "Random Forest": rfc, #Probar en ocotillo
    "SVM": svc, #Probar en ocotillo
    "Lasso": lasso_clf,
    "Gradient Boosting": gbc, #Probar en ocotillo
}

bow_vectorizer = CountVectorizer()
tfidf_vectorizer = TfidfVectorizer()

X_train_bow = bow_vectorizer.fit_transform(docs_train)
X_train_tfidf = tfidf_vectorizer.fit_transform(docs_train)

vectorizers = {
    "tfidf" : X_train_bow,
    "bow" : X_train_tfidf,
    ### Agregar mas
}

from sklearn.model_selection import cross_val_score

scores = []

for vectorizer_name, vectorizer in vectorizers.items():
    for model_name, model in models.items():
        start_time = time.time()

        X_train_vectorized = vectorizer
        # f1_macro para tomar en cuenta el desbalance
        score = cross_val_score(model, X_train_vectorized, y_train, scoring='f1_macro', n_jobs=-1, cv=5) # Cambiar esto si estas en el ocotillo
        mean_score = score.mean()
        scores.append((vectorizer_name, model_name, mean_score))

        end_time = time.time()
        elapsed_time = end_time - start_time
        print(f"{vectorizer_name}, {model_name} : done in {elapsed_time:.2f} seconds")

score_df = pd.DataFrame(scores)

score_df.columns = ['Vectorizer', 'Estimator', 'TF Score']

score_df.to_csv('results_no_hyper.csv', index=False, )